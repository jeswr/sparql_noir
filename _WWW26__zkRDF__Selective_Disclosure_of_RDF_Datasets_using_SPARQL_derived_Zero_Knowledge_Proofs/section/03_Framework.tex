\section{The zkRDF Approach}
\label{sec:framework}

In this section, we introduce \textit{zkRDF}, a data-centric approach for selectively disclosing SPARQL query results from RDF datasets using ZKPs.
We first present the common abstract methodology (Section~\ref{sec:framework_abstract}) that underlies its conceptual instantiation (Section~\ref{sec:framework_instantiation}) which is a direct extension of the work presented in \cite{DBLP:conf/esws/BraunK25}.
We provide an initial formalisation in Appendix~\ref{appendix:zkrdf}.
A proof-of-concept implementation is available online\footnote{\url{https://anonymous.4open.science/r/rdf-zkp-sparql}}.

\subsection{Abstract Methodology}
\label{sec:framework_abstract}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.99\linewidth]{img/zk-SPARQL-horizontal.png}
    \caption{An illustration of zkRDF's abstract methodology to achieve query-derived selective disclosure. Rounded nodes represent data, gray boxes represent procedures. Edge labels qualify the link between data and a procedure.}
    \Description{Diagram showing zkRDF methodology with rounded nodes for data, gray boxes for procedures, and labeled edges.}
    \label{fig:methodology}
\end{figure*}
The central idea of the zkRDF approach is the systematic transformation of a SPARQL query and its solution into a set of relations, statements, and witnesses to be proven.
This approach, illustrated in Figure~\ref{fig:methodology}, consists of the following procedures:

\begin{enumerate}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,wide,labelwidth=!,labelindent=0pt,leftmargin=14pt]   
    \item[\textbf{Query Execution.}] The query engine executes a query and produces a set of solution mappings.
    
    \item[\textbf{Query Analysis.}] The query is dissected and relations, statements and the selection of witnesses are derived:
    \begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,wide,labelwidth=!,labelindent=0pt,leftmargin=14pt]
        \item[Solution Modifiers] specify which variable mappings are to be projected.
        In the solution analysis, it is thus determined which variable mappings should made public and which should remain private.
        \item[Graph Patterns] specify the structure of the data to be matched, \ie what data is relevant and where relationships must exist.
        Graph Patterns thus entail relations and corresponding statements for proofs of knowledge of signatures on data matching the respective patterns.
        \item[Expressions] specify logical and arithmetic conditions that the matched data must satisfy. This determines how solutions are validated or constrained.
        Expressions thus entail relations and corresponding statements for \eg proofs of numeric bounds.
    \end{itemize}
    
    \item[ \textbf{Solution Analysis.}] Based on the selected variables to project and the solution mappings, it is determined which RDF terms are made public, \ie are part of the statements, and which remain private, \ie are witnesses.

    \item[\textbf{Proof Creation.}] The proof system creates a proof from the set of relations, statements, and corresponding witnesses.
    Moreover, the system provides a serialization of the composite proof to be presented to a verifier.
\end{enumerate}





\subsection{Conceptual Instantiation}
\label{sec:framework_instantiation}

We present one approach to instantiate the abstract methodology for selectively disclosing an RDF dataset using SPARQL-derived ZKPs.
Our approach is based on both the semantics and the techniques presented in~\cite{DBLP:conf/esws/BraunK25}.
In this approach,  
the results of an input SPARQL query are indirectly proven to be cryptographically correct via selective disclosure of the queried dataset.

That is, in contrast to the computation-centric approaches, which produce a proof of solution mappings, this data-centric approach provides proofs about the underlying dataset such that the verifier can obtain the query results from the selectively disclosed dataset.
The presented dataset thus needs to include cryptographically verifiable data for each solution mapping to be obtained by the verifier. 

\paragraph{On the data holder's (prover) side.}
%
To produce such a selectively disclosing dataset, 
we need determine for each solution mapping:
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,wide,labelwidth=!,labelindent=0pt,leftmargin=14pt]
\item[(a)] which RDF terms in which quads of the dataset are part of that solution mapping and should thus be revealed, 
\item[(b)] which RDF terms in which quads of the dataset helped produce that solution mapping by matching the query's graph pattern but are kept hidden, and 
\item[(c)] which of the hidden RDF terms fulfill \texttt{FILTER} expressions from the query that need to be proven.
\end{itemize}
This requires tracing the \enquote{origins} of a solution mapping, which includes tracing all non-disclosed variables and blank nodes in the graph patterns.
This particular approach is comprised of the following steps; formalised in Appendix~\ref{appendix:zkrdf}:
\begin{enumerate}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,wide,labelwidth=!,labelindent=0pt,leftmargin=14pt]
\begin{comment}
    \item[\textit{1. Rewrite Query:}]
    Any blank node in a graph pattern is replaced with a fresh and unique variable. 
    Additionally, all variables are to be projected.
    This is to trace all variables involved in the query logic, not only the ones the verifier is interested in.\todo{JW: remove this step - it is just implementing SPARQL semantics}
\end{comment}

    \item[\textit{1. Execute Query:}]
     The query is executed against the RDF dataset to obtain the set of solutions mappings.

    \item[\textit{2. Trace Each Mapping (before projection):}]
    For each solution mapping, trace every mapped RDF term back to its occurrence in the underlying RDF dataset. 
    This includes intermediate variables, \ie non-projected variables and all blank nodes in the query's graph patterns.

    \item[\textit{3. Derive Relations, Statements and Witnesses:}]~
    In the scope of this work, we derive from the query, its solution and its trace:
    \begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,wide,labelwidth=!,labelindent=0pt,leftmargin=14pt]
        \item[{Proofs of Knowledge of Signature.}]
        For each solution mapping, for each of the datasets graphs that are relevant to the SPARQL query, a corresponding proof of knowledge of signature on that graph is to be added to the proof composition.
    
        \item[{Proofs of Numeric Bounds.}]
        Inspect the query's \texttt{FILTER} expressions to extract relational expressions that indicate numeric constraints.
        If the variable in the filter expression is not projected, a corresponding proof of numeric bounds is to be added to the proof composition.

        \item[{Equality Constraints.}]
        When a particular variable-mapped RDF term remains hidden, equivalence of the hidden term across its occurrences relevant to the SPARQL query is proven.
    \end{itemize}
    For each relation, determine which RDF terms should be revealed as part of the proof statement and which should be kept hidden as witnesses.
    Terms mapped to variables to be projected are to be revealed.
    All other mapped terms are to be hidden.
    Constants from the query are revealed; they are already known to the verifier who formulated the query.
    
    \item[\textit{4. Create Proof and Presentation:}]
    Given the relations, statements and witnesses create a composite proof and generate a corresponding presentation of the selectively disclosed dataset.
\end{enumerate}

\noindent
The presented approach instantiates the abstract methodology as follows:
Query execution is in direct correspondence; the query is executed \eg using a regular SPARQL query engine.
Tracing each solution mapping is conceptually part of the solution analysis procedure.
Technically, tracing may be implemented during query execution. %; \eg as suggested by our formalization provided in Appendix~\ref{appendix:zkrdf}.
Deriving relations, statements and witnesses instantiates parts of both query analysis and solution analysis. 
Separation of concern between the two analysis procedures is not completely clear cut in this instance.
Finally, creating the proof and presentation is in direct correspondence.% \todo{I don't get this section mapping to the abstract methodology - I suggest remove or clarify}

In Appendix~\ref{appendix:zkrdf}, we provide a more formal description this instantiation based on extending the SPARQL query evaluation to directly provide the required tracing information during query execution.

\paragraph{On the verifier's (consumer) side.}
%
The verifier receives the presentation from the prover.
To obtain query results, the verifier executes the SPARQL query or a re-written version of it on the presentation.

In case of a simple SPARQL query that does not have a FILTER statement for numeric bounds, the verifier can directly execute the query:
For each solution mapping that the prover obtained from the original dataset, the individual graphs in the selectively disclosing dataset are entailed from the underlying respective graphs in original dataset under simple entailment~\cite{DBLP:conf/esws/BraunK25}.
This means that the graph patterns that the SPARQL query defines are preserved and thus will find the corresponding matches as if executed on the original dataset.

In case of a SPARQL query that includes \texttt{FILTER} statements for numeric bounds, we need to rewrite the query if the variables used in the \texttt{FILTER} statements are not projected.
If they were projected, then the corresponding values are revealed and the verifier can check the \texttt{FILTER} statement themselves.
If they are indeed not projected, then the \texttt{FILTER} statement will filter out the blank node that acts as a stand-in for the hidden literal.
In this case, the \texttt{FILTER} statement is replaced by a graph pattern that specifies a proof of numeric bounds where the bounds correspond to what was specified in the \texttt{FILTER} and the witness is specified using the variable from the \texttt{FILTER}\todo{JW: This needs wordsmithing to improve clarity}.
At this point, it is sufficient for the SPARQL query to only check for existence of a proof of numeric bounds which proves that the property specified in the \texttt{FILTER} holds.
Verifying that the property is indeed proven, \ie that the \texttt{FILTER} was indeed correctly applied, is part of the validity check~\cite{DBLP:conf/esws/BraunK25} on the selectively disclosing dataset.
%\todo{JW: Do you define what a validity check is in this paper - if not spend 1-2 sentences doing so}.











\begin{comment}
We summarize the main aspects of this approach:
\begin{itemize}
\item[{Goal:}] Prove that a private dataset contains solution mappings to a query.
\item[{Components:}]  Query engine, query-to-claim compiler, composite proof system
\item[{Inputs:}]      SPARQL query, RDF dataset incl. issuers' public keys and signatures
\item[{Process:}]    Rewrite and execute query, compile claims from analysis of query and solution, create proofs about the dataset to prove correctness of solution
\item[{Output:}]    Selectively disclosed dataset incl. composite proof
\end{itemize}
\end{comment}


\begin{comment}
    graph TD

%% Define styles for clarity
%% Data: Unfilled (white), stadium shape (highly rounded)
%% Process: Filled (gray), sharp rectangle
classDef data fill:#fff,stroke:#333,stroke-width:2px;
classDef process fill:#ddd,stroke:#333,stroke-width:2px;
classDef none fill:none,stroke:none;

   
    subgraph inputs [ ]
        Q([SPARQL Query])
        DS([Private Dataset])
    end

    subgraph querying [ ]
        QE["Query Engine"]
        Q -->|"is input to"| QE
        DS -->|"is input to"| QE
        SM([Solution Mappings])
        QE -->|"produces"| SM
    end
        
    subgraph analysis [ ]
        QA["Query Analysis"]
        Q -->|"is input to"| QA 
        QP([Selected Variables])
        QA -->|"extracts"| QP
        QP -->|"is input to"| SA

        SA["Solution Analysis"]
        SM -->|"is input to"| SA

        R(["Relations<br/>(to prove)"])
        S(["Statements<br/>(public inputs)"])
        W(["Witnesses<br/>(secret inputs)"])   
        QA -->|"derives"| R
        QA -->|"extracts"| S
        SA -->|"reveales"| S
        SA -->|"hides"| W
    end

subgraph proves [ ]
PS["Proof System"]
    R -->|"is input to"| PS
    S -->|"is input to"| PS
    W -->|"is input to"| PS

    P(["Composite Proof<br/>& Presentation"])
    PS -->|creates| P
end

%% Apply the styles
class Q,QP,DS,R,S,W,SM,P data;
class QA,QE,PS,SA process;
class inputs,proves none;
 class querying,analysis none;

\end{comment}


\begin{comment}

It is thus the question which particular proofs about the underlying dataset need to be composed such that a verifier would obtain correct query results.
To provide an intuition:
The query in Listing~\ref{listing:run-example-query}, a simple example, specifies a graph pattern resembling a membership card and a FILTER statement asking that the membership is not yet expired.
Moreover, the organization that the membership is attested of is to be projected; the membership's user or its end date do not need to be disclosed.
\lstset{style=sparql}
\begin{lstlisting}[
    language=SPARQL, 
    caption={A simple query to illustrated how properties to be proven are derived.},
    label={listing:run-example-query}, 
    basicstyle=\scriptsize\ttfamily,
]
SELECT ?org WHERE {
  ?membership org:member ?user.
  ?membership org:organization ?org .
  ?membership time:hasEnd ?endDate .
  FILTER(?endDate > xsd:dateTime("2025-10-01T00:00:00Z") )
}
\end{lstlisting}
A solution to this query would then entails a corresponding proof that the triples matching the graph pattern had indeed been signed by an issuer, \eg the organisation itself, and a proof of numeric bounds that the end date, which remains hidden, is indeed in the future.
Listing~\ref{listing:run-example-present} provides an example of a resulting selectively disclosing dataset.
Note \eg that blank node \texttt{\_:0\_12} is both object of the triple in line 4 and in line 16, meaning that the underlying literal is proven to be  part of the membership card and that the same value is proven to be within numeric bounds.
\lstset{style=turtle}
\begin{lstlisting}[
    language=Turtle, 
    caption={result P} ,
    label={listing:run-example-present}, 
    basicstyle=\scriptsize\ttfamily
]
GRAPH _:0_4 {
	_:0_0 _:0_1 _:0_2 .
	_:0_5 _:0_6 _:0_7 .
	_:0_10 time:hasEnd _:0_12 .
	_:0_10 org:member _:0_17 .
	_:0_10 org:organization <http://example.org/organisations#aCompany> .
}
GRAPH _:presentationProofGraph {
	_:cproof rdf:type zkp:CompositeProof .
	_:cproof zkp:hasComponent _:p0 .
	_:cproof zkp:hasComponent _:p1 .
	_:p0 rdf:type bbsp16:PoKS16 . # proof of knowledge of signature
    _:p0 bbsp16:isProofOfKnowledgeOfSignatureOverGraph _:0_4 .
    # more proof details omitted for brevity
	_:p1 rdf:type lg16:LegoGroth16ProofOfRangeMembership . # numeric bounds proof
    _:p1 lg16:hasWitness _:0_12 .
	_:p1 lg16:hasLowerBound "1759276800"^^xsd:nonNegativeInteger .
	_:p1 lg16:hasUpperBound "18446744073709551615"^^xsd:nonNegativeInteger .
	# more proof details omitted for brevity
}
\end{lstlisting}
\end{comment}